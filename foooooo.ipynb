{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc8028-4d2a-47cc-b21c-b4991a63ee3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44dc89e-6aa3-4016-bb25-fb27202ea485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fedbd-b4f5-4942-9cf7-30dce3245142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Universal Sentence Encoder model\n",
    "model_url = \"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\"\n",
    "model = hub.load(model_url)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Define a list of taste-related words for each taste category\n",
    "taste_words = {\n",
    "    'sweet': ['sweet', 'sugary', 'honeyed', 'saccharine', 'caramel'],\n",
    "    'sour': ['sour', 'tart', 'acidic', 'citrusy', 'vinegary'],\n",
    "    'salty': ['salty', 'briny', 'savor', 'sodium', 'seawater'],\n",
    "    'bitter': ['bitter', 'astringent', 'sharp', 'acrid', 'pungent'],\n",
    "    'umami': ['umami', 'savory', 'meaty', 'brothy', 'glutamate']\n",
    "}\n",
    "\n",
    "def embed(texts):\n",
    "    return model(texts)\n",
    "\n",
    "def classify_taste(sentence):\n",
    "    # Tokenize \n",
    "    words = word_tokenize(sentence.lower())\n",
    "\n",
    "    # Initialize dictionaries to store counts of taste words\n",
    "    taste_counts = {taste: 0 for taste in taste_words}\n",
    "\n",
    "    # Count occurrences of taste words in the sentence\n",
    "    for word in words:\n",
    "        for taste, taste_list in taste_words.items():\n",
    "            if word in taste_list:\n",
    "                taste_counts[taste] += 1\n",
    "\n",
    "    # Find the predominant taste(s)\n",
    "    predominant_tastes = [taste for taste, count in taste_counts.items() if count > 0]\n",
    "\n",
    "    if predominant_tastes:\n",
    "        return predominant_tastes\n",
    "    else:\n",
    "        return \"No taste-related words found in the sentence.\"\n",
    "\n",
    "def recommend(text):\n",
    "    emb = embed([text])\n",
    "    neighbors = nn.kneighbors(emb, return_distance=False)[0]\n",
    "    return df[[\"Food\"]].iloc[neighbors].values.tolist()\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(\"fooddetails1 - Sheet1 (1).csv\", engine=\"python\")\n",
    "df = df[[\"Food\", \"Taste\", \"Combination\"]].dropna().reset_index()[:5500]\n",
    "\n",
    "# Embed food titles\n",
    "titles = list(df[\"Taste\"])\n",
    "embeddings = embed(titles)\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Train k-nearest neighbors model\n",
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "nn.fit(embeddings)\n",
    "\n",
    "# Example usage:\n",
    "sentence = input(\"Enter your taste to eat: \")\n",
    "output = classify_taste(sentence)\n",
    "\n",
    "if output == \"No taste-related words found in the sentence.\":\n",
    "    print(\"No taste-related words found in the sentence.\")\n",
    "else:\n",
    "    output = ', '.join(output)\n",
    "    recm = recommend(output)\n",
    "    for sublist in recm:\n",
    "        for pred_food in sublist:\n",
    "            print()\n",
    "            print(\"Food:\", pred_food)\n",
    "            for index, row in df.iterrows():\n",
    "                food = row['Food']\n",
    "                if pred_food == food:\n",
    "                    print(\"Combination:\", row['Combination'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e9bc2-1849-462e-9e6d-e710035c6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fooddetails1 - Sheet1 (1).csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e864808-a4a2-46d6-889b-e77deb362e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eldos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Shape of embeddings: (199, 512)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Suppress TensorFlow deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Download NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Universal Sentence Encoder model\n",
    "model_url = \"http://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(model_url)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Define a list of taste-related words for each taste category\n",
    "taste_words = {\n",
    "      \"sweety\":[\"sweet\",\"sweety\",\"sugary\"],\"sugary\":[\"sugary\"],\"honeyed\":[\"honeyed\"],'saccharine':['saccharine'],\n",
    "    'caramelized':['caramelized'],'fruity':['fruity'],\"sour\":['sour'],'tangy':[\"tangy\"],\"acidic\":[\"acidic\"],\n",
    "    \"tart\":[\"tart\"],\"citrusy\":[\"citrusy\"],\"salty\":[\"salt\",\"salty\"],\"spicy\":[\"spicy\",\"spice\"],\"briny\":[\"briny\"],\n",
    "    \"savory\":[\"savory\"],\"bitter\":[\"bitter\"],\"astringent\":[\"astringent\"],\"umami\":['umami'],\"meaty\":[\"meaty\"],\n",
    "    \"full-bodied\":[\"full-bodied\"],\"soft\":[\"soft\"],\"creamy\":[\"creamy\"],\"smooth\":[\"smooth\"],\"tender\":[\"tender\"],\n",
    "    \"crispy\":[\"crispy\"],\"crunchy\":[\"crunchy\"],\"crumbly\":[\"crumbly\"],\"brittle\":[\"brittle\"],\"chewy\":[\"chewy\"],\n",
    "    \"gummy\":[\"gummy\"],\"elastic\":[\"elastic\"],\"taffy\":[\"taffy\"],\"gelatinous\":[\"gelatinous\"],\"jelly\":[\"jelly\"],\n",
    "    \"wobbly\":[\"wobbly\"],\"soft\":[\"soft\"],\"fluffy\":[\"fluffy\"],\"light\":[\"light\"],\"spongy\":[\"spongy\",\"sponge\"],\"delicate\":[\"delicate\"],\n",
    "    \"steam\":[\"steaming\",\"steam\"]\n",
    "}\n",
    "\n",
    "def embed(texts):\n",
    "    return model(texts)\n",
    "\n",
    "def classify_taste(sentence):\n",
    "    # Tokenize the input sentence\n",
    "    words = word_tokenize(sentence.lower())\n",
    "\n",
    "    # Initialize dictionaries to store counts of taste words\n",
    "    taste_counts = {taste: 0 for taste in taste_words}\n",
    "\n",
    "    # Count occurrences of taste words in the sentence\n",
    "    for word in words:\n",
    "        for taste, taste_list in taste_words.items():\n",
    "            if word in taste_list:\n",
    "                taste_counts[taste] += 1\n",
    "\n",
    "    # Find the predominant taste(s)\n",
    "    predominant_tastes = [taste for taste, count in taste_counts.items() if count > 0]\n",
    "\n",
    "    if predominant_tastes:\n",
    "        return predominant_tastes\n",
    "    else:\n",
    "        return \"No taste-related words found in the sentence.\"\n",
    "\n",
    "def recommend(text):\n",
    "    emb = embed([text])\n",
    "    neighbors = nn.kneighbors(emb, return_distance=False)[0]\n",
    "    return df[[\"Food\"]].iloc[neighbors].values.tolist()\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(\"fooddetails2.csv\", engine=\"python\")\n",
    "df = df[[\"Food\", \"Taste\", \"Combination\"]].dropna().reset_index()[:5500]\n",
    "\n",
    "# Embed food titles\n",
    "titles = list(df[\"Taste\"])\n",
    "embeddings = embed(titles)\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Train k-nearest neighbors model\n",
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "nn.fit(embeddings)\n",
    "\n",
    "# Save trained models and DataFrame to pickle files\n",
    "with open('models.pickle', 'wb') as f:\n",
    "    pickle.dump({'nn': nn, 'pca': pca, 'df': df}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d2a2937-84f0-4536-a0a2-4705cbb045df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trying to load a model of incompatible/unknown type. 'C:\\Users\\user\\AppData\\Local\\Temp\\tfhub_modules\\37809451352f132df4e32b8fcaf6c8c522dba01b' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load Universal Sentence Encoder model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://tfhub.dev/google/universal-sentence-encoder/4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Define a list of taste-related words for each taste category\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py:113\u001b[0m, in \u001b[0;36mload\u001b[1;34m(handle, tags, options)\u001b[0m\n\u001b[0;32m    108\u001b[0m saved_model_pbtxt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    109\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(module_path),\n\u001b[0;32m    110\u001b[0m     tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(saved_model_path) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(saved_model_pbtxt_path)):\n\u001b[1;32m--> 113\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load a model of incompatible/unknown type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m contains neither \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m nor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    115\u001b[0m                    (module_path, tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PB,\n\u001b[0;32m    116\u001b[0m                     tf\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options:\n\u001b[0;32m    119\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(tf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoadOptions\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Trying to load a model of incompatible/unknown type. 'C:\\Users\\user\\AppData\\Local\\Temp\\tfhub_modules\\37809451352f132df4e32b8fcaf6c8c522dba01b' contains neither 'saved_model.pb' nor 'saved_model.pbtxt'."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Suppress TensorFlow deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Download NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Universal Sentence Encoder model\n",
    "model_url = \"http://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(model_url)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Define a list of taste-related words for each taste category\n",
    "taste_words = {\n",
    "     \"sweety\":[\"sweet\",\"sweety\",\"sugary\"],\"sugary\":[\"sugary\"],\"honeyed\":[\"honeyed\"],'saccharine':['saccharine'],\n",
    "    'caramelized':['caramelized'],'fruity':['fruity'],\"sour\":['sour'],'tangy':[\"tangy\"],\"acidic\":[\"acidic\"],\n",
    "    \"tart\":[\"tart\"],\"citrusy\":[\"citrusy\"],\"salty\":[\"salt\",\"salty\"],\"spicy\":[\"spicy\",\"spice\"],\"briny\":[\"briny\"],\n",
    "    \"savory\":[\"savory\"],\"bitter\":[\"bitter\"],\"astringent\":[\"astringent\"],\"umami\":['umami'],\"meaty\":[\"meaty\"],\n",
    "    \"full-bodied\":[\"full-bodied\"],\"soft\":[\"soft\"],\"creamy\":[\"creamy\"],\"smooth\":[\"smooth\"],\"tender\":[\"tender\"],\n",
    "    \"crispy\":[\"crispy\"],\"crunchy\":[\"crunchy\"],\"crumbly\":[\"crumbly\"],\"brittle\":[\"brittle\"],\"chewy\":[\"chewy\"],\n",
    "    \"gummy\":[\"gummy\"],\"elastic\":[\"elastic\"],\"taffy\":[\"taffy\"],\"gelatinous\":[\"gelatinous\"],\"jelly\":[\"jelly\"],\n",
    "    \"wobbly\":[\"wobbly\"],\"soft\":[\"soft\"],\"fluffy\":[\"fluffy\"],\"light\":[\"light\"],\"spongy\":[\"spongy\",\"sponge\"],\"delicate\":[\"delicate\"],\n",
    "    \"steam\":[\"steaming\",\"steam\"]\n",
    "}\n",
    "\n",
    "def embed(texts):\n",
    "    return model(texts)\n",
    "\n",
    "def classify_taste(sentence):\n",
    "    # Tokenize the input sentence\n",
    "    words = word_tokenize(sentence.lower())\n",
    "\n",
    "    # Initialize dictionaries to store counts of taste words\n",
    "    taste_counts = {taste: 0 for taste in taste_words}\n",
    "\n",
    "    # Count occurrences of taste words in the sentence\n",
    "    for word in words:\n",
    "        for taste, taste_list in taste_words.items():\n",
    "            if word in taste_list:\n",
    "                taste_counts[taste] += 1\n",
    "\n",
    "    # Find the predominant taste(s)\n",
    "    predominant_tastes = [taste for taste, count in taste_counts.items() if count > 0]\n",
    "\n",
    "    if predominant_tastes:\n",
    "        return predominant_tastes\n",
    "    else:\n",
    "        return \"No taste-related words found in the sentence.\"\n",
    "\n",
    "def recommend(text):\n",
    "    emb = embed([text])\n",
    "    neighbors = nn.kneighbors(emb, return_distance=False)[0]\n",
    "    return df[[\"Food\"]].iloc[neighbors].values.tolist()\n",
    "\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(\"fooddetails2.csv\", engine=\"python\")\n",
    "df = df[[\"Food\", \"Taste\", \"Combination\"]].dropna().reset_index()[:5500]\n",
    "\n",
    "# Embed food titles\n",
    "titles = list(df[\"Taste\"])\n",
    "embeddings = embed(titles)\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Train k-nearest neighbors model\n",
    "nn = NearestNeighbors(n_neighbors=6)\n",
    "nn.fit(embeddings)\n",
    "\n",
    "# Save trained models and DataFrame to pickle files\n",
    "with open('foody.pickle', 'wb') as f:\n",
    "    pickle.dump({'nn': nn, 'pca': pca, 'df': df}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2cfc12-fb1e-4786-a7ea-7208925aea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "model_url = \"http://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(model_url)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767de210-b68f-4076-b851-583ab3076fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\users\\eldos\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\eldos\\anaconda3\\lib\\site-packages (from kagglehub) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\eldos\\anaconda3\\lib\\site-packages (from kagglehub) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\eldos\\anaconda3\\lib\\site-packages (from requests->kagglehub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\eldos\\anaconda3\\lib\\site-packages (from requests->kagglehub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\eldos\\anaconda3\\lib\\site-packages (from requests->kagglehub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eldos\\anaconda3\\lib\\site-packages (from requests->kagglehub) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\eldos\\anaconda3\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706a7f97-2d97-475b-8a3b-d0f701db5303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/google/universal-sentence-encoder/tensorFlow2/cmlm-en-base/1/download...\n",
      "Resuming download from 278921216 bytes (128913212 bytes left)...\n",
      "100%|██████████| 389M/389M [00:58<00:00, 2.22MB/s]\n",
      "Extracting model files...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: C:\\Users\\eldos\\.cache\\kagglehub\\models\\google\\universal-sentence-encoder\\tensorFlow2\\cmlm-en-base\\1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"google/universal-sentence-encoder/tensorFlow2/cmlm-en-base\")\n",
    "\n",
    "print(\"Path to model files:\", path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
